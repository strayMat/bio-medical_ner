{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "import random\n",
    "random.seed(12)\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils_paper/')\n",
    "from conllUtils import write_trainfiles, describe_entities, group_conll, describe_entities\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and dev corpus construction for conll2003 corpus\n",
    "\n",
    "We build a train and dev set from the [conll2003 corpus](https://www.clips.uantwerpen.be/conll2003/ner/) with a complient format for the neural tagger [YASET](http://yaset.readthedocs.io/en/stable/). This corpus contains 592 documents that we convert to conll files (suitable input format for the yaset tool). We got it from [Lample](https://github.com/glample/tagger/tree/master/dataset) (see [Lample et al., 2016](https://arxiv.org/abs/1603.01360))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_train = '../data/iob_data/eng.train'\n",
    "iob_dev = '../data/iob_data/eng.testa'\n",
    "iob_test = '../data/iob_data/eng.testb'\n",
    "\n",
    "path2train = '../data/train.conll'\n",
    "path2dev = '../data/dev.conll'\n",
    "path2test = '../data/test.conll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace space between columns by tabulations to be complient with YASET format\n",
    "with open(iob_train, 'r') as f:\n",
    "    train_data = f.read().splitlines()\n",
    "train_data = [re.sub(' ', '\\t', l)+'\\n' for l in train_data]\n",
    "with open(path2train, 'w') as f:\n",
    "    f.writelines(train_data)\n",
    "\n",
    "with open(iob_dev, 'r') as f:\n",
    "    dev_data = f.read().splitlines()\n",
    "dev_data = [re.sub(' ', '\\t', l)+'\\n' for l in dev_data]\n",
    "with open(path2dev, 'w') as f:\n",
    "    f.writelines(dev_data)\n",
    "\n",
    "with open(iob_test, 'r') as f:\n",
    "    test_data = f.read().splitlines()\n",
    "test_data = [re.sub(' ', '\\t', l)+'\\n' for l in test_data]\n",
    "with open(path2test, 'w') as f:\n",
    "    f.writelines(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From iob to bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter\\tNNP\\tI-NP\\tB-PER', 'Blackburn\\tNNP\\tI-NP\\tI-PER', '']\n",
      "['Peter\\tNNP\\tI-NP\\tB-PER\\n', 'Blackburn\\tNNP\\tI-NP\\tI-PER\\n', '\\n']\n",
      "['.\\t.\\tO\\tO', '', 'LONDON\\tNNP\\tI-NP\\tB-LOC']\n",
      "['.\\t.\\tO\\tO\\n', '\\n', 'LONDON\\tNNP\\tI-NP\\tB-LOC\\n']\n",
      "['DEFEAT\\tNN\\tI-NP\\tO', '.\\t.\\tO\\tO', '']\n",
      "['DEFEAT\\tNN\\tI-NP\\tO\\n', '.\\t.\\tO\\tO\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "## the tagging scheme in the orignial dataset is not exactely bio so we have to convert it\n",
    "grouping = {'PER':['PER'], 'ORG':['ORG'], 'LOC':['LOC'], 'MISC':['MISC']}\n",
    "_ = group_conll(path2train, path2train, grouping)\n",
    "_ = group_conll(path2dev, path2dev, grouping)\n",
    "_ = group_conll(path2test, path2test, grouping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: \n",
      "Corpus containing 204567 tokens in 14986 sentences with 34043 non-O tags\n",
      "Counter({'O': 170524, 'PER': 11128, 'ORG': 10025, 'LOC': 8297, 'MISC': 4593}) \n",
      "\n",
      "Dev set: \n",
      "Corpus containing 51578 tokens in 3465 sentences with 8603 non-O tags\n",
      "Counter({'O': 42975, 'PER': 3149, 'LOC': 2094, 'ORG': 2092, 'MISC': 1268}) \n",
      "\n",
      "Test set: \n",
      "Corpus containing 46666 tokens in 3683 sentences with 8112 non-O tags\n",
      "Counter({'O': 38554, 'PER': 2773, 'ORG': 2496, 'LOC': 1925, 'MISC': 918}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train set: ')\n",
    "print(describe_entities(path2train, verbose = True), '\\n')\n",
    "print('Dev set: ')\n",
    "print(describe_entities(path2dev, verbose = True), '\\n')\n",
    "print('Test set: ')\n",
    "print(describe_entities(path2test, verbose = True), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tags= 42646, prop= 0.16649163559702512\n",
      "nb_tokens= 256145, nb_sent 18451\n"
     ]
    }
   ],
   "source": [
    "tot_tagged = 34043 + 8603\n",
    "nb_tokens = 51578 + 204567\n",
    "nb_sents = 3465 + 14986\n",
    "print('nb_tags= {}, prop= {}'.format(tot_tagged, tot_tagged/nb_tokens))\n",
    "print('nb_tokens= {}, nb_sent {}'.format(nb_tokens, nb_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

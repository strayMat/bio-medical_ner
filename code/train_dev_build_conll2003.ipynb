{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re \n",
    "import os\n",
    "import random\n",
    "random.seed(12)\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "sys.path.insert(1, '../../utils/')\n",
    "\n",
    "from conllUtils import write_trainfiles, describe_entities, group_conll, describe_entities\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and dev corpus construction for conll2003 corpus\n",
    "\n",
    "We build a train and dev set from the [conll2003 corpus](https://www.clips.uantwerpen.be/conll2003/ner/). This corpus contains 592 documents that we convert to conll files (suitable input format for the yaset tool). We got it from [Lample](https://github.com/glample/tagger/tree/master/dataset) (see [Lample et al., 2016](https://arxiv.org/abs/1603.01360))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2lample_train = '../data/lample_conll2003_datasets/eng.train'\n",
    "path2lample_dev = '../data/lample_conll2003_datasets/eng.testa'\n",
    "path2lample_test = '../data/lample_conll2003_datasets/eng.testb'\n",
    "\n",
    "path2train = '../data/train.conll'\n",
    "path2dev = '../data/dev.conll'\n",
    "path2test = '../data/test.conll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace space between columns by tabulations to be complient with yaset format\n",
    "with open(path2lample_train, 'r') as f:\n",
    "    train_data = f.read().splitlines()\n",
    "train_data = [re.sub(' ', '\\t', l)+'\\n' for l in train_data]\n",
    "with open(path2train, 'w') as f:\n",
    "    f.writelines(train_data)\n",
    "\n",
    "with open(path2lample_dev, 'r') as f:\n",
    "    dev_data = f.read().splitlines()\n",
    "dev_data = [re.sub(' ', '\\t', l)+'\\n' for l in dev_data]\n",
    "with open(path2dev, 'w') as f:\n",
    "    f.writelines(dev_data)\n",
    "\n",
    "with open(path2lample_test, 'r') as f:\n",
    "    test_data = f.read().splitlines()\n",
    "test_data = [re.sub(' ', '\\t', l)+'\\n' for l in test_data]\n",
    "with open(path2test, 'w') as f:\n",
    "    f.writelines(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From iob to bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter\\tNNP\\tI-NP\\tI-PER', 'Blackburn\\tNNP\\tI-NP\\tI-PER', '']\n",
      "['Peter\\tNNP\\tI-NP\\tB-PER\\n', 'Blackburn\\tNNP\\tI-NP\\tI-PER\\n', '\\n']\n",
      "['.\\t.\\tO\\tO', '', 'LONDON\\tNNP\\tI-NP\\tI-LOC']\n",
      "['.\\t.\\tO\\tO\\n', '\\n', 'LONDON\\tNNP\\tI-NP\\tB-LOC\\n']\n",
      "['DEFEAT\\tNN\\tI-NP\\tO', '.\\t.\\tO\\tO', '']\n",
      "['DEFEAT\\tNN\\tI-NP\\tO\\n', '.\\t.\\tO\\tO\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "grouping = {'PER':['PER'], 'ORG':['ORG'], 'LOC':['LOC'], 'MISC':['MISC']}\n",
    "_ = group_conll(path2train, '../data/bio_train.conll', grouping)\n",
    "_ = group_conll(path2dev, '../data/bio_dev.conll', grouping)\n",
    "_ = group_conll(path2test, '../data/bio_test.conll', grouping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus containing 204567 tokens in 14986 sentences with 34043 non-O tags\n",
      "Counter({'O': 170524, 'PER': 11128, 'ORG': 10025, 'LOC': 8297, 'MISC': 4593})\n",
      "Corpus containing 51578 tokens in 3465 sentences with 8603 non-O tags\n",
      "Counter({'O': 42975, 'PER': 3149, 'LOC': 2094, 'ORG': 2092, 'MISC': 1268})\n",
      "Corpus containing 46666 tokens in 3683 sentences with 8112 non-O tags\n",
      "Counter({'O': 38554, 'PER': 2773, 'ORG': 2496, 'LOC': 1925, 'MISC': 918})\n"
     ]
    }
   ],
   "source": [
    "print(describe_entities(path2train, verbose = True))\n",
    "print(describe_entities(path2dev, verbose = True))\n",
    "print(describe_entities(path2test, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1665859375\n",
      "42646\n"
     ]
    }
   ],
   "source": [
    "tot_tagged = 34043 + 8603\n",
    "print(tot_tagged/256000)\n",
    "print(tot_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "random.seed(12)\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "sys.path.insert(1, '../../utils_paper/')\n",
    "\n",
    "from bratUtils import myCorpus_brat2conll\n",
    "from conllUtils import describe_entities\n",
    "from myDocClass import ncbi_doc\n",
    "\n",
    "from collections import Counter\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and dev corpus construction for NCBI-disease corpus\n",
    "\n",
    "We build a train and dev set from the [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/). This corpus contains 592 documents that we convert to conll files (suitable input format for the yaset tool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2train = '../data/src/NCBItrainset_corpus.txt'\n",
    "path2train_write = '../data/ncbi_train'\n",
    "path2dev = '../data/src/NCBIdevelopset_corpus.txt'\n",
    "path2dev_write = '../data/ncbi_dev'\n",
    "path2test = '../data/src/NCBItestset_corpus.txt'\n",
    "path2test_write = '../data/ncbi_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path2train, 'r') as f:\n",
    "    train_data = f.read().splitlines()\n",
    "with open(path2dev, 'r') as f:\n",
    "    dev_data = f.read().splitlines()\n",
    "with open(path2test, 'r') as f:\n",
    "    test_data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build document objects from the brat-like format of the orginal data\n",
    "train_docs = []\n",
    "point = 0\n",
    "entities = []\n",
    "for l in train_data[1:]:\n",
    "    if l == '':\n",
    "        train_docs.append(ncbi_doc(title, text, entities))\n",
    "        point = 0\n",
    "        entities = []\n",
    "    else:\n",
    "        if point == 0:\n",
    "            title = l\n",
    "        elif point == 1:\n",
    "            text = l\n",
    "        else:\n",
    "            entities.append(l)\n",
    "        point += 1\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_docs = []\n",
    "point = 0\n",
    "entities = []\n",
    "for l in dev_data[1:]:\n",
    "    if l == '':\n",
    "        dev_docs.append(ncbi_doc(title, text, entities))\n",
    "        point = 0\n",
    "        entities = []\n",
    "    else:\n",
    "        if point == 0:\n",
    "            title = l\n",
    "        elif point == 1:\n",
    "            text = l\n",
    "        else:\n",
    "            entities.append(l)\n",
    "        point += 1\n",
    "len(dev_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = []\n",
    "point = 0\n",
    "entities = []\n",
    "for l in test_data[1:]:\n",
    "    if l == '':\n",
    "        test_docs.append(ncbi_doc(title, text, entities))\n",
    "        point = 0\n",
    "        entities = []\n",
    "    else:\n",
    "        if point == 0:\n",
    "            title = l\n",
    "        elif point == 1:\n",
    "            text = l\n",
    "        else:\n",
    "            entities.append(l)\n",
    "        point += 1\n",
    "len(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x7fe46b9552e8>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x7fe46b8faa40>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x7fe46b8faa98>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifying nlp tokenizer to be consistent \n",
    "def create_custom_tokenizer(nlp):\n",
    "    \n",
    "    my_prefix = [r'[0-9]\\.']\n",
    "    \n",
    "    all_prefixes_re = spacy.util.compile_prefix_regex(tuple(list(nlp.Defaults.prefixes) + my_prefix))\n",
    "    \n",
    "    # Handle ( that doesn't have proper spacing around it\n",
    "    custom_infixes = ['\\.\\.\\.+', '(?<=[0-9])-(?=[0-9])','(\\w\\w*)-(\\w\\w*)', '[!&:,()]']\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
    "    \n",
    "    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)   \n",
    "    \n",
    "    return Tokenizer(nlp.vocab, nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search = all_prefixes_re.search, \n",
    "                     infix_finditer = infix_re.finditer, suffix_search = suffix_re.search,\n",
    "                     token_match=None)\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio #dev/(#dev + #train) = 99/691 = 0.143\n"
     ]
    }
   ],
   "source": [
    "print('ratio #dev/(#dev + #train) = {}/{} = {:.3}'.format\\\n",
    "      (len(dev_docs), len(dev_docs) + len(train_docs), len(dev_docs)/(len(dev_docs) + len(train_docs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/592 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592 documents. Processing conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 592/592 [00:39<00:00, 14.86it/s]\n",
      "  2%|▏         | 2/99 [00:00<00:05, 17.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 documents. Processing conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:06<00:00, 14.61it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = myCorpus_brat2conll(train_docs, nlp, path2train_write)\n",
    "_ = myCorpus_brat2conll(dev_docs, nlp, path2dev_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/99 [00:00<00:13,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 documents. Processing conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:06<00:00, 15.21it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = myCorpus_brat2conll(test_docs, nlp, path2test_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying that there are no missing entities after conversion from brat-like to conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'DiseaseClass': 1708,\n",
       "         'SpecificDisease': 5756,\n",
       "         'CompositeMention': 558,\n",
       "         'Modifier': 1755})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brat_ents = []\n",
    "for doc in train_docs:\n",
    "    for ent in doc.entities:\n",
    "        for w in ent.word.split(' '):\n",
    "            brat_ents.append(ent.label)\n",
    "Counter(brat_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'DiseaseClass': 247,\n",
       "         'SpecificDisease': 841,\n",
       "         'Modifier': 328,\n",
       "         'CompositeMention': 160})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brat_ents = []\n",
    "for doc in dev_docs:\n",
    "    for ent in doc.entities:\n",
    "        for w in ent.word.split(' '):\n",
    "            brat_ents.append(ent.label)\n",
    "Counter(brat_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Modifier': 360,\n",
       "         'SpecificDisease': 1072,\n",
       "         'DiseaseClass': 244,\n",
       "         'CompositeMention': 78})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brat_ents = []\n",
    "for doc in test_docs:\n",
    "    for ent in doc.entities:\n",
    "        for w in ent.word.split(' '):\n",
    "            brat_ents.append(ent.label)\n",
    "Counter(brat_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus containing 128557 tokens in 6233 sentences with 9775 non-O tags\n",
      "Counter({'O': 118782, 'SpecificDisease': 5755, 'Modifier': 1755, 'DiseaseClass': 1708, 'CompositeMention': 557}) \n",
      "\n",
      "Corpus containing 22508 tokens in 1046 sentences with 1575 non-O tags\n",
      "Counter({'O': 20933, 'SpecificDisease': 840, 'Modifier': 328, 'DiseaseClass': 247, 'CompositeMention': 160}) \n",
      "\n",
      "Corpus containing 22956 tokens in 1058 sentences with 1754 non-O tags\n",
      "Counter({'O': 21202, 'SpecificDisease': 1072, 'Modifier': 360, 'DiseaseClass': 244, 'CompositeMention': 78})\n",
      "0.07516556291390729\n",
      "11350\n"
     ]
    }
   ],
   "source": [
    "path2test = '../data/ncbi_test.conll'\n",
    "path2dev = '../data/ncbi_dev.conll'\n",
    "path2conll = '../data/ncbi_train.conll'\n",
    "\n",
    "print(describe_entities(path2conll), '\\n')\n",
    "print(describe_entities(path2dev), '\\n')\n",
    "print(describe_entities(path2test))\n",
    "tot_tagged = 9775 + 1575\n",
    "print(tot_tagged/151000)\n",
    "print(tot_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tags= 11350, prop= 0.07513322079899382\n",
      "nb_tokens= 151065, nb_sent 7279\n"
     ]
    }
   ],
   "source": [
    "tot_tagged = 9775 + 1575\n",
    "nb_tokens = 128557 + 22508\n",
    "nb_sents = 6233 + 1046\n",
    "print('nb_tags= {}, prop= {}'.format(tot_tagged, tot_tagged/nb_tokens))\n",
    "print('nb_tokens= {}, nb_sent {}'.format(nb_tokens, nb_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
